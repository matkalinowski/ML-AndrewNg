{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import winsound\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load input data containing digits in mat format\n",
    "digits = sio.loadmat('data/ex4data1.mat')\n",
    "\n",
    "#save input and result of data\n",
    "X = pd.DataFrame(digits['X'])\n",
    "y = pd.get_dummies(pd.DataFrame(digits['y'])[0])\n",
    "y.columns = list(range(0, 10))\n",
    "\n",
    "#load computed weights\n",
    "weights = sio.loadmat('data/ex4weights.mat')\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_theta(params, eps=.12):\n",
    "    \"\"\"Randomly initialize the weights to small values\n",
    "    Parameters\n",
    "    ------------\n",
    "    params : array\n",
    "        List of arrays of theta to be initialized.\n",
    "    eps : float\n",
    "        Given weights will be initialized in range (-eps, eps),\n",
    "        default: 0.12.\n",
    "    Returns\n",
    "    -----------\n",
    "    init_theta : array,\n",
    "        Array of len as given in params containing values in range (-eps, eps)\n",
    "    \"\"\" \n",
    "    init_theta = []\n",
    "    for theta in params:\n",
    "        init_theta.append(np.random.random(theta.shape)*2*eps-eps)\n",
    "    return init_theta\n",
    "\n",
    "def reshapeParams(flattened_array, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "    \"\"\"Reshapes flattened array and returns Theta **transposed** values \n",
    "        in respect of Neural network layer sizes. Given version is created \n",
    "        for layer with one hidden layer.\n",
    "    Parameters\n",
    "    ------------\n",
    "    flattened_array : array\n",
    "        Flattened array of Theta values.\n",
    "    input_layer_size : array\n",
    "        Size of input layer\n",
    "    hidden_layer_size : array\n",
    "        Size of hidden layer\n",
    "    output_layer_size : array\n",
    "        Size of output layer\n",
    "    Returns\n",
    "    -----------\n",
    "    init_theta : array, len = 2\n",
    "        Theta **transposed** values \n",
    "        in respect of Neural network layer sizes\n",
    "    \"\"\" \n",
    "    theta1 = flattened_array[:(input_layer_size + 1) * hidden_layer_size] \\\n",
    "        .reshape((input_layer_size + 1, hidden_layer_size))\n",
    "    theta2 = flattened_array[(input_layer_size + 1) * hidden_layer_size:] \\\n",
    "        .reshape((hidden_layer_size + 1, output_layer_size))\n",
    "    return [theta1, theta2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients without regularization, calculated:\n",
      " [[ 0.766138 -0.02754  -0.024929]\n",
      " [ 0.979897 -0.035844 -0.053862]]\n",
      "Expected:\n",
      " [[ 0.766138 -0.02754  -0.024929]\n",
      " [ 0.979897 -0.035844 -0.053862]]\n",
      "No regularization gradients are the same:\n",
      "[[ True  True  True]\n",
      " [ True  True  True]]\n",
      "Cost is equal to expected with toleration 0.001: [ True]\n",
      "Cost is equal to: [7.40696985606575], expected is: 7.407\n",
      "\n",
      "Gradients without regularization, calculated:\n",
      " [[ 0.766138  0.37246   0.641738]\n",
      " [ 0.979897  0.497489  0.746138]]\n",
      "Expected:\n",
      " [[ 0.76614  0.37246  0.64174]\n",
      " [ 0.9799   0.49749  0.74614]]\n",
      "Check if gradients without regularization are the same:\n",
      "[[ True  True  True]\n",
      " [ True  True  True]]\n",
      "Cost is equal to expected with toleration 0.001: [False]\n",
      "Cost is equal to: [19.47363652273242], expected is: 19.474\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\" Implementation of Feedforward Neural Network with one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, lam, theta_params=None):\n",
    "        self.lam = lam\n",
    "        self.w1, self.w2 = self._initialize_weights(theta_params)\n",
    "        self.cost = []\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        sig = self._sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def _add_bias_unit(self, df, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            x_new = np.ones((df.shape[0], df.shape[1] + 1))\n",
    "            x_new[:, 1:] = df\n",
    "        elif how == 'row':\n",
    "            x_new = np.ones((df.shape[0] + 1, df.shape[1]))\n",
    "            x_new[1:, :] = df\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        return x_new\n",
    "\n",
    "    def _feed_forward(self, X):\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = a1.dot(self.w1)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='column')\n",
    "        z3 = a2.dot(self.w2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "\n",
    "        self.a = [a1, a2, a3]\n",
    "        self.z = [z2, z3]\n",
    "        return self.a, self.z\n",
    "\n",
    "    def _calculate_cost(self, y, output, m):\n",
    "        term1 = np.multiply(y.T, np.log(output))\n",
    "        term2 = np.multiply((1 - y.T), np.log(1 - output))\n",
    "        cost = term1 + term2\n",
    "        reg_term = self._L2_reg(self.lam, self.w1, self.w2)\n",
    "        reg_term = reg_term / m\n",
    "        return -np.sum(np.sum(cost)) / m + reg_term\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        return (lambda_ / 2.0) * (np.sum(w1[1:, :] ** 2) + np.sum(w2[1:, :] ** 2))\n",
    "\n",
    "    def _backPropagate(self, y, m):\n",
    "        a3 = self.a[-1]\n",
    "        sigma3 = a3 - y\n",
    "\n",
    "        z2 = self.z[0]\n",
    "        sigma2 = sigma3.dot(self.w2[1:, :].T) * self._sigmoid_gradient(z2)\n",
    "\n",
    "        a1 = self.a[0]\n",
    "        a2 = self.a[1]\n",
    "\n",
    "        grad1 = sigma2.T.dot(a1)\n",
    "        grad2 = sigma3.T.dot(a2)\n",
    "\n",
    "        theta_grad1 = grad1 / m\n",
    "        theta_grad2 = grad2 / m\n",
    "        theta_grad1 = np.array(theta_grad1)\n",
    "        theta_grad2 = np.array(theta_grad2)\n",
    "\n",
    "        grad1_reg = (self.lam / m) * self.w1[1:, :].T\n",
    "        theta_grad1[:, 1:] += grad1_reg\n",
    "\n",
    "        grad2_reg = (self.lam / m) * self.w2[1:, :].T\n",
    "        theta_grad2[:, 1:] += grad2_reg\n",
    "\n",
    "        return [theta_grad1, theta_grad2]\n",
    "\n",
    "    def fit(self, X, y, epochs, display_progres=False):\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        m = X.shape[0]\n",
    "\n",
    "        for i in range(1, epochs + 1):\n",
    "            if display_progres & (i%10 == 0):\n",
    "                print(f'Performing calculations for network: {i}/{epochs}')\n",
    "\n",
    "            self._feed_forward(X_data)\n",
    "            nn_output = pd.DataFrame(self.a[-1]).T\n",
    "            self.gradients = nn._backPropagate(y, m)\n",
    "            self.cost.append(self._calculate_cost(y_data, nn_output, m))\n",
    "            self.w1 -= self.gradients[0].T\n",
    "            self.w2 -= self.gradients[1].T\n",
    "            \n",
    "        self.nn_output = nn_output\n",
    "        return self\n",
    "\n",
    "    def _initialize_weights(self, theta_params):\n",
    "        return theta_params[0].copy(), theta_params[1].copy()\n",
    "\n",
    "input_layer_size = 2\n",
    "hidden_layer_size = 2\n",
    "output_layer_size = 4\n",
    "params = np.array(list(range(1, 19))) / 10  # nn_params\n",
    "theta_params = reshapeParams(params, input_layer_size, hidden_layer_size, output_layer_size)\n",
    "\n",
    "X_t = pd.DataFrame(np.cos([[1, 2], [3, 4], [5, 6]]))\n",
    "y_t = np.array([4, 2, 3])\n",
    "y_t = pd.get_dummies(y_t)\n",
    "zeros = np.zeros(y_t.shape[0])\n",
    "y_t.insert(0, value=zeros, column='1')\n",
    "\n",
    "lam = 0\n",
    "\n",
    "nn = NeuralNetwork(lam, theta_params)\n",
    "network_fitted = nn.fit(X_t, y_t, 1)\n",
    "gradients = network_fitted.gradients\n",
    "nn_output = network_fitted.nn_output\n",
    "cost = network_fitted.cost\n",
    "net_a = network_fitted.a\n",
    "net_z = network_fitted.z\n",
    "\n",
    "GRADIENTS_0_EXPECTED_NO_REG = np.array([[0.766138, -0.02754, -0.024929],\n",
    "                                        [0.979897, -0.035844, -0.053862]])\n",
    "EXPECTED_COST_NO_REG = 7.4070\n",
    "TOL = 6\n",
    "ARR_TOL = 1.e-6\n",
    "COST_TOL = 1.e-3\n",
    "\n",
    "gr = gradients[0]\n",
    "gr = np.round(gr, TOL)\n",
    "print('Gradients without regularization, calculated:\\n', gr)\n",
    "print('Expected:\\n', GRADIENTS_0_EXPECTED_NO_REG)\n",
    "no_reg_gradients_close_cond = np.isclose(gr, GRADIENTS_0_EXPECTED_NO_REG, atol=ARR_TOL)\n",
    "print(f'No regularization gradients are the same:\\n{no_reg_gradients_close_cond}')\n",
    "\n",
    "cost_close = np.isclose(cost, EXPECTED_COST_NO_REG, atol=COST_TOL)\n",
    "print(f'Cost is equal to expected with toleration {COST_TOL}: {cost_close}\\n'\n",
    "      f'Cost is equal to: {cost}, expected is: {EXPECTED_COST_NO_REG}')\n",
    "\n",
    "\n",
    "GRADIENTS_0_EXPECTED_WITH_REG = np.array([[ 0.76614,  0.37246,  0.64174],\n",
    "                                        [ 0.9799 ,  0.49749,  0.74614]])\n",
    "EXPECTED_COST_WITH_REG = 19.474\n",
    "lam = 4\n",
    "\n",
    "nn = NeuralNetwork(lam, theta_params)\n",
    "network_fitted = nn.fit(X_t, y_t, 1)\n",
    "gradients = network_fitted.gradients\n",
    "cost = network_fitted.cost\n",
    "gr = gradients[0]\n",
    "gr = np.round(gr, TOL)\n",
    "\n",
    "print()\n",
    "print('Gradients without regularization, calculated:\\n', gr)\n",
    "print('Expected:\\n', GRADIENTS_0_EXPECTED_WITH_REG)\n",
    "no_reg_gradients_close_cond = np.isclose(gr, GRADIENTS_0_EXPECTED_WITH_REG, atol=ARR_TOL)\n",
    "print(f'Check if gradients without regularization are the same:\\n{no_reg_gradients_close_cond}')\n",
    "\n",
    "cost_close = np.isclose(cost, EXPECTED_COST_NO_REG, atol=COST_TOL)\n",
    "print(f'Cost is equal to expected with toleration {COST_TOL}: {cost_close}\\n'\n",
    "      f'Cost is equal to: {cost}, expected is: {EXPECTED_COST_WITH_REG}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function should equal 0.287629 without regularization, computed cost is: [0.28762916516131881]\n",
      "Cost function should equal 0.38377 with regularization, computed cost is: [0.38376985909092354]\n"
     ]
    }
   ],
   "source": [
    "COST_FUNC_WITH_REG = 0.383770\n",
    "COST_FUNC_WITHOUT_REG = 0.287629\n",
    "theta_params = [Theta1.T, Theta2.T]\n",
    "\n",
    "lam = 0\n",
    "nn = NeuralNetwork(lam, theta_params)\n",
    "ff = nn.fit(X, y, 1)\n",
    "print(f'Cost function should equal {COST_FUNC_WITHOUT_REG} without regularization, computed cost is: {ff.cost}')\n",
    "np.testing.assert_almost_equal(ff.cost, COST_FUNC_WITHOUT_REG, decimal=6, err_msg='Gradient checking failed.')\n",
    "\n",
    "lam = 1\n",
    "nn = NeuralNetwork(lam, theta_params)\n",
    "ff = nn.fit(X, y, 1)\n",
    "print(f'Cost function should equal {COST_FUNC_WITH_REG} with regularization, computed cost is: {ff.cost}')\n",
    "np.testing.assert_almost_equal(ff.cost, COST_FUNC_WITH_REG, decimal=6, err_msg='Gradient checking failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9752\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lam = 0\n",
    "\n",
    "theta_params = [Theta1.T, Theta2.T]\n",
    "m = X.shape[0]\n",
    "\n",
    "nn = NeuralNetwork(lam, theta_params)\n",
    "network_fitted = nn.fit(X, y, 1)\n",
    "\n",
    "gradients = network_fitted.gradients\n",
    "nn_output = network_fitted.nn_output\n",
    "nn_output.head(2)\n",
    "\n",
    "y_true = pd.get_dummies(pd.DataFrame(digits['y'])) - 1\n",
    "y_pred = nn_output.idxmax(axis=0).values\n",
    "print('acc:', accuracy_score(y_true, y_pred))\n",
    "\n",
    "winsound.PlaySound('sound.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9394\n",
      "Wall time: 24.8 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f76ff40978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lam = 0\n",
    "\n",
    "theta_params = [Theta1.T, Theta2.T]\n",
    "init_theta = initialize_theta(theta_params)\n",
    "m = X.shape[0]\n",
    "\n",
    "nn = NeuralNetwork(lam, init_theta)\n",
    "network_fitted = nn.fit(X, y, 500)\n",
    "\n",
    "gradients = network_fitted.gradients\n",
    "nn_output = network_fitted.nn_output\n",
    "nn_cost = network_fitted.cost\n",
    "nn_output.head(2)\n",
    "\n",
    "y_true = pd.get_dummies(pd.DataFrame(digits['y'])) - 1\n",
    "y_pred = nn_output.idxmax(axis=0).values\n",
    "print('acc:', accuracy_score(y_true, y_pred))\n",
    "plt.plot(range(0, len(nn_cost)), nn_cost)\n",
    "winsound.PlaySound('sound.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9348\n",
      "Wall time: 24.3 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJFRsEtIlJkFNwiIkXmPwHSJoQpNMUlsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f7717189b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lam = 4\n",
    "\n",
    "theta_params = [Theta1.T, Theta2.T]\n",
    "init_theta = initialize_theta(theta_params)\n",
    "m = X.shape[0]\n",
    "\n",
    "nn = NeuralNetwork(lam, init_theta)\n",
    "network_fitted = nn.fit(X, y, 500)\n",
    "\n",
    "gradients = network_fitted.gradients\n",
    "nn_output = network_fitted.nn_output\n",
    "nn_cost = network_fitted.cost\n",
    "nn_output.head(2)\n",
    "\n",
    "y_true = pd.get_dummies(pd.DataFrame(digits['y'])) - 1\n",
    "y_pred = nn_output.idxmax(axis=0).values\n",
    "print('acc:', accuracy_score(y_true, y_pred))\n",
    "plt.plot(range(0, len(nn_cost)), nn_cost)\n",
    "winsound.PlaySound('sound.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>num_res</th>\n",
       "      <th>backprop_res</th>\n",
       "      <th>norm_diff(num-grad)</th>\n",
       "      <th>relative_error</th>\n",
       "      <th>num-backprop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(225, 13)</td>\n",
       "      <td>-0.00011151</td>\n",
       "      <td>-0.00011151</td>\n",
       "      <td>2.77436e-09</td>\n",
       "      <td>2.77436e-09</td>\n",
       "      <td>6.1874e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(155, 1)</td>\n",
       "      <td>8.87448e-05</td>\n",
       "      <td>8.87812e-05</td>\n",
       "      <td>0.000205103</td>\n",
       "      <td>0.000205103</td>\n",
       "      <td>3.6411e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(95, 14)</td>\n",
       "      <td>0.00023778</td>\n",
       "      <td>0.000237771</td>\n",
       "      <td>1.96435e-05</td>\n",
       "      <td>1.96435e-05</td>\n",
       "      <td>9.34148e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(57, 20)</td>\n",
       "      <td>-2.02853e-06</td>\n",
       "      <td>-2.02846e-06</td>\n",
       "      <td>1.55354e-05</td>\n",
       "      <td>1.55354e-05</td>\n",
       "      <td>6.3027e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(36, 10)</td>\n",
       "      <td>7.09273e-07</td>\n",
       "      <td>7.09429e-07</td>\n",
       "      <td>0.000109923</td>\n",
       "      <td>0.000109923</td>\n",
       "      <td>1.55948e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(183, 8)</td>\n",
       "      <td>-3.32833e-05</td>\n",
       "      <td>-3.32844e-05</td>\n",
       "      <td>1.6089e-05</td>\n",
       "      <td>1.6089e-05</td>\n",
       "      <td>1.07101e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(90, 14)</td>\n",
       "      <td>2.0094e-05</td>\n",
       "      <td>2.0101e-05</td>\n",
       "      <td>0.00017383</td>\n",
       "      <td>0.00017383</td>\n",
       "      <td>6.98712e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(342, 24)</td>\n",
       "      <td>-1.96979e-07</td>\n",
       "      <td>-1.9699e-07</td>\n",
       "      <td>2.80558e-05</td>\n",
       "      <td>2.80558e-05</td>\n",
       "      <td>1.10531e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(109, 9)</td>\n",
       "      <td>-0.000153753</td>\n",
       "      <td>-0.000153738</td>\n",
       "      <td>4.76664e-05</td>\n",
       "      <td>4.76664e-05</td>\n",
       "      <td>1.4657e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(30, 11)</td>\n",
       "      <td>-2.45311e-06</td>\n",
       "      <td>-2.45315e-06</td>\n",
       "      <td>6.77247e-06</td>\n",
       "      <td>6.77247e-06</td>\n",
       "      <td>3.32275e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index      num_res backprop_res norm_diff(num-grad) relative_error  \\\n",
       "0  (225, 13)  -0.00011151  -0.00011151         2.77436e-09    2.77436e-09   \n",
       "0   (155, 1)  8.87448e-05  8.87812e-05         0.000205103    0.000205103   \n",
       "0   (95, 14)   0.00023778  0.000237771         1.96435e-05    1.96435e-05   \n",
       "0   (57, 20) -2.02853e-06 -2.02846e-06         1.55354e-05    1.55354e-05   \n",
       "0   (36, 10)  7.09273e-07  7.09429e-07         0.000109923    0.000109923   \n",
       "0   (183, 8) -3.32833e-05 -3.32844e-05          1.6089e-05     1.6089e-05   \n",
       "0   (90, 14)   2.0094e-05   2.0101e-05          0.00017383     0.00017383   \n",
       "0  (342, 24) -1.96979e-07  -1.9699e-07         2.80558e-05    2.80558e-05   \n",
       "0   (109, 9) -0.000153753 -0.000153738         4.76664e-05    4.76664e-05   \n",
       "0   (30, 11) -2.45311e-06 -2.45315e-06         6.77247e-06    6.77247e-06   \n",
       "\n",
       "  num-backprop  \n",
       "0   6.1874e-13  \n",
       "0   3.6411e-08  \n",
       "0  9.34148e-09  \n",
       "0   6.3027e-11  \n",
       "0  1.55948e-10  \n",
       "0  1.07101e-09  \n",
       "0  6.98712e-09  \n",
       "0  1.10531e-11  \n",
       "0   1.4657e-08  \n",
       "0  3.32275e-11  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.2262126392626516e-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.2262126392626516e-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.8731505885008939e-09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def calculate_cost(X, y, size, lam, params):\n",
    "    return NeuralNetwork(lam, params).fit(X, y, 1).cost\n",
    "\n",
    "def get_random_index_of_ndarray(array):\n",
    "    return random.randint(0, array.shape[0]-1), \\\n",
    "           random.randint(0, array.shape[1] - 1)\n",
    "\n",
    "def perform_gradient_checking(X, y, backpropag, theta_params):\n",
    "    eps = .0001\n",
    "    lam = 0\n",
    "    data_frames = []\n",
    "    m = X.shape[0]\n",
    "    t1 = theta_params[0]\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        rand_index = get_random_index_of_ndarray(t1)\n",
    "        t1[rand_index] = t1[rand_index] + eps\n",
    "        left = calculate_cost(X, y, m, lam, theta_params)[0]\n",
    "        t1[rand_index] = t1[rand_index] - 2*eps\n",
    "        right = calculate_cost(X, y, m, lam, theta_params)[0]\n",
    "        num_res = (left - right)/(2*eps)\n",
    "        backprop_res = backpropag[0].T[rand_index]\n",
    "        norm = np.linalg.norm(num_res - backprop_res) / np.linalg.norm(num_res + backprop_res)\n",
    "        norm1 = np.linalg.norm(num_res - backprop_res)\n",
    "        norm2 = np.linalg.norm(num_res + backprop_res)\n",
    "        norm_diff = norm1/norm2\n",
    "        \n",
    "        norm2 = np.linalg.norm(num_res)\n",
    "        norm3 = np.linalg.norm(backprop_res)\n",
    "        relative_error = norm1 / (norm2 + norm3)\n",
    "        data_frames.append(pd.DataFrame([str(rand_index), num_res, backprop_res,\n",
    "                                         norm_diff, relative_error, np.linalg.norm(num_res- backprop_res)]).T)\n",
    "\n",
    "    df = pd.concat(data_frames)\n",
    "    df.columns = ['Index', 'num_res', 'backprop_res', 'norm_diff(num-grad)', 'relative_error', 'num-backprop']\n",
    "    return df\n",
    "\n",
    "theta_params = [Theta1.T, Theta2.T]\n",
    "lam = 0\n",
    "nn = NeuralNetwork(lam, theta_params)\n",
    "\n",
    "network_fitted = nn.fit(X, y, 1)\n",
    "gradients = network_fitted.gradients\n",
    "nn_output = network_fitted.nn_output\n",
    "\n",
    "gradient_df = perform_gradient_checking(X, y, gradients, theta_params)\n",
    "gradient_df\n",
    "gradient_df.iloc[:,-3].mean()\n",
    "gradient_df.iloc[:,-2].mean()\n",
    "gradient_df.iloc[:,-1].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}